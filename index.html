<!DOCTYPE HTML>
<html lang="en">
  <head>
	<meta name="google-site-verification" content="aJr13PvMsUeLTnyKuVvnIXus1QhKWSCSvUoHV-LXXu8" />
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Ido Galil</title>

    <meta name="author" content="Ido Galil">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-J3W42XSHWJ"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-J3W42XSHWJ');
    </script>

    <!-- Minimal JS for "Read More" toggles -->
    <script>
      function toggleSummary(id) {
        const summaryDiv = document.getElementById(id);
        if (summaryDiv.style.display === "none") {
          summaryDiv.style.display = "block";
        } else {
          summaryDiv.style.display = "none";
        }
      }
    </script>
    
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
      <tbody>
        <tr style="padding:0px">
          <td style="padding:0px">

            <!-- Top / Bio Section -->
            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
              <tbody>
                <tr style="padding:0px">
                  <td style="padding:2.5%;width:63%;vertical-align:middle">
                    <p class="name" style="text-align: center;">
                      Ido Galil
                    </p>
                    <p>
                      I am a Deep Learning Researcher in the Deci group at NVIDIA (formerly Deci AI, acquired by NVIDIA).
                      My work focuses on improving the inference efficiency of large language models (LLMs) and generative AI.
                      I graduated my PhD under Prof. Ran El-Yaniv at the 
                      <a href="https://www.cs.technion.ac.il/" target="_blank" rel="noopener noreferrer">CS faculty</a>,
                      <a href="https://www.technion.ac.il/en" target="_blank" rel="noopener noreferrer">Technion</a>.
                      In my PhD research, I studied deep neural networks’ reliability and safety in computer vision and natural language processing, 
                      with an emphasis on uncertainty estimation, selective prediction, and adversarial robustness.
                    </p>
                    <p style="text-align:center">
                      <a href="mailto:idogalil@campus.technion.ac.il">Email</a> &nbsp;/&nbsp;
                      <a href="https://scholar.google.com/citations?user=eZA2cu8AAAAJ&hl" target="_blank" rel="noopener noreferrer">Scholar</a> &nbsp;/&nbsp;
                      <a href="https://linkedin.com/in/ido-galil" target="_blank" rel="noopener noreferrer">LinkedIn</a>
                    </p>
                  </td>
                  <td style="padding:2.5%;width:40%;max-width:40%">
                    <a href="data/me_2024.jpeg">
                      <img style="width:100%;max-width:100%;object-fit:cover;border-radius:50%;" 
                           alt="profile photo" 
                           src="data/me_2024.jpeg" loading="lazy">
                    </a>
                  </td>
                </tr>
              </tbody>
            </table>

            <!-- Publications Section -->
            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
              <tbody>
                <tr>
                  <td style="padding:20px;width:100%;vertical-align:middle">
                    <h2>Publications</h2>
                  </td>
                </tr>
              </tbody>
            </table>

            <!-- Papers List -->
            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
              <tbody>

                <!-- FFN Fusion (NeurIPS 2025 Spotlight) -->
                <tr>
                  <td style="padding:20px;width:35%;vertical-align:middle">
                    <img src="images/ffn_fusion.jpg" alt="FFN Fusion" width="100%" loading="lazy">
                  </td>
                  <td style="padding:20px;width:65%;vertical-align:middle">
                    <p class="papertitle">FFN Fusion: Rethinking Sequential Computation in Large Language Models</p>
                    <p>
                      Authors: Akhiad Bercovich · Mohammad Dabbah · Omri Puny · Ido Galil · Amnon Geifman · Yonatan Geifman · Izhak Golan · Ehud Karpas · Itay Levy · Zach Moshe · Najeeb Nabwani · Tomer Ronen · Itamar Schen · Elad Segal · Ido Shahaf · Oren Tropp · Ran Zilberstein · Ran El-Yaniv
                    </p>
                    <p>
                      <em>
                        <img src="images/neurips_logo.png" alt="NeurIPS" style="height:36px;vertical-align:middle;margin-right:5px;" loading="lazy">
                        NeurIPS, 2025 (Spotlight)
                        &nbsp;&nbsp;
                        <img src="images/nvidia_logo.jpg" alt="NVIDIA" style="height:30px;vertical-align:middle;margin-right:5px;" loading="lazy">
                      </em>
                    </p>
                    <p><strong>TL;DR</strong>: FFN Fusion fuses consecutive FFN layers into larger blocks, reducing sequential depth and accelerating inference with minimal accuracy impact.</p>
                    <a href="javascript:void(0);" onclick="toggleSummary('ffnfusion-summary')" style="color:blue;text-decoration:underline;">Read More</a>
                    <div id="ffnfusion-summary" style="display:none;margin-top:10px;">
                      <p>
                        We run Puzzle to search hardware‑aware designs, then fuse consecutive FFNs into larger FFNs, decreasing the model's depth. Across models from tens to hundreds of billions of parameters, FFN Fusion reduces latency and cost while preserving quality, and complements techniques such as quantization and pruning.
                      </p>
                    </div>
                    <p>
                      <a href="https://arxiv.org/pdf/2503.18908v1" target="_blank" rel="noopener noreferrer">Paper</a>
                    </p>
                  </td>
                </tr>

                <!-- New Paper (NAACL 2025) -->
                <tr>
                  <td style="padding:20px;width:35%;vertical-align:middle">
                    <img src="images/flux_pad.jpeg" alt="Padding Tone" width="100%" loading="lazy">
                  </td>
                  <td style="padding:20px;width:65%;vertical-align:middle">
                    <p class="papertitle">Padding Tone: A Mechanistic Analysis of Padding Tokens in T2I Models</p>
                    <p>
                      Authors: Michael Toker · Ido Galil · Hadas Orgad · Rinon Gal · Yoad Tewel · Gal Chechik · Yonatan Belinkov
                    </p>
                    <p>
                      <em>
                        <img src="images/acl.png" alt="NAACL" style="height:30px;vertical-align:middle;margin-right:5px;" loading="lazy">
                        NAACL, 2025 (Oral)
                        &nbsp;&nbsp;
                        <img src="images/nvidia_logo.jpg" alt="NVIDIA" style="height:30px;vertical-align:middle;margin-right:5px;" loading="lazy">
                        <img src="images/technion_logo.png" alt="Technion" style="height:30px;vertical-align:middle;margin-right:5px;" loading="lazy">
                      </em>
                    </p>
                    <p><strong>TL;DR</strong>: Our work reveals how text-to-image (T2I) diffusion models use “empty” padding tokens, which can still influence generated images depending on model architecture and training.</p>
                    <a href="javascript:void(0);" onclick="toggleSummary('paddingtone-summary')" style="color:blue;text-decoration:underline;">Read More</a>
                    <div id="paddingtone-summary" style="display:none;margin-top:10px;">
                      <p>
                        In T2I pipelines, prompts are padded to a fixed length with a special pad token—normally ignored by language models. However, we find T2I models may treat them differently. We develop two causal intervention methods in the text encoder and the diffusion process to test whether these tokens carry semantic information. Experiments on six T2I models reveal scenarios where padding tokens are ignored, or become semantically significant, or even serve as “registers” that store and recall data during diffusion. Our findings underscore the need for more precise handling of padding tokens in T2I model design.
                      </p>
                    </div>
                    <p>
                      <a href="https://arxiv.org/pdf/2501.06751" target="_blank" rel="noopener noreferrer">Paper</a>
                    </p>
                  </td>
                </tr>

                <!-- Synthetic Cells (Advanced Biology - Cover) -->
                <tr>
                  <td style="padding:20px;width:30%;vertical-align:middle">
                    <img src="images/synthetic_cell.png" alt="Synthetic Cells" width="80%" style="display:block;margin:0 auto;" loading="lazy">
                  </td>
                  <td style="padding:20px;width:70%;vertical-align:middle">
                    <p class="papertitle">Scaling Up Synthetic Cell Production Using Robotics and Machine Learning Toward Therapeutic Applications</p>
                    <p>
                      Authors: Noga Sharf-Pauker · Ido Galil · Omer Kfir · Gal Chen · Rotem Menachem · Jeny Shklover · Avi Schroeder · Shanny Ackerman
                    </p>
                    <p>
                      <em>
                        <img src="images/Advanced_Logo.jpg" alt="Advanced Biology" style="height:16px;vertical-align:middle;margin-right:5px;" loading="lazy">
                        Advanced Biology, 2025 (Journal Cover)
                      </em>
                    </p>
                    <p><strong>TL;DR</strong>: We couple robotics with machine learning to optimize and monitor synthetic cell production. We use deep neural networks to assess the synthetic cells' quality.</p>
                    <a href="javascript:void(0);" onclick="toggleSummary('advbio-summary')" style="color:blue;text-decoration:underline;">Read More</a>
                    <div id="advbio-summary" style="display:none;margin-top:10px;">
                      <p>
                        An automated robotics-and-ML pipeline scales synthetic cell production with robust quality control; deep neural networks support quality assessment and assurance toward therapeutic-grade manufacturing.
                      </p>
                    </div>
                    <p>
                      <a href="https://advanced.onlinelibrary.wiley.com/doi/10.1002/adbi.202400671" target="_blank" rel="noopener noreferrer">Paper</a>
                    </p>
                  </td>
                </tr>

                <!-- Puzzle (ArXiv 2024, NVIDIA) -->
                <tr>
                  <td style="padding:20px;width:45%;vertical-align:middle">
                    <img src="images/puzzle_overview.png" alt="Puzzle Overview" width="100%" loading="lazy">
                  </td>
                  <td style="padding:20px;width:55%;vertical-align:middle">
                    <p class="papertitle">
                      Puzzle: Distillation-Based NAS for Inference-Optimized LLMs
                    </p>
                    <p>
                      Authors: Akhiad Bercovich · Tomer Ronen · Talor Abramovich · Nir Ailon · Nave Assaf · 
                      Mohammad Dabbah · Ido Galil · Amnon Geifman · Yonatan Geifman · Izhak Golan · 
                      Netanel Haber · Ehud Karpas · Roi Koren · Itay Levy · Pavlo Molchanov · Shahar Mor · 
                      Zach Moshe · Najeeb Nabwani · Omri Puny · Ran Rubin · Itamar Schen · Ido Shahaf · 
                      Oren Tropp · Omer Ullman Argov · Ran Zilberstein · Ran El-Yaniv
                    </p>
                    <p>
                      <em>
                        <img src="images/ICML-logo.svg" alt="ICML" style="height:30px;vertical-align:middle;margin-right:5px;" loading="lazy">
                        ICML, 2025
                        &nbsp;&nbsp;
                        <img src="images/nvidia_logo.jpg" alt="NVIDIA" style="height:30px;vertical-align:middle;margin-right:5px;" loading="lazy">
                      </em>
                    </p>
                    <p><strong>TL;DR</strong>: Puzzle accelerates LLM inference on specific hardware by leveraging blockwise local knowledge distillation and mixed-integer programming to preserve model performance while significantly reducing inference costs.</p>
                    <a href="javascript:void(0);" onclick="toggleSummary('puzzle-summary')" style="color:blue;text-decoration:underline;">Read More</a>
                    <div id="puzzle-summary" style="display:none;margin-top:10px;">
                      <p>
                        Despite LLMs’ impressive results, they are often limited by computational costs during inference. 
                        Puzzle addresses this by optimizing large-scale models for specific hardware without sacrificing accuracy, 
                        resulting in up to 2.17× speedups.
                      </p>
                    </div>
                    <p>
                      <a href="https://openreview.net/pdf?id=RY5MMBHRqo" target="_blank" rel="noopener noreferrer">Paper</a> /
                      <a href="https://www.youtube.com/watch?v=YsIv9Kr99C4" target="_blank" rel="noopener noreferrer">Video</a> /
                      <a href="https://github.com/NVlabs/puzzle" target="_blank" rel="noopener noreferrer">Code</a>
                    </p>
                  </td>
                </tr>

                

                

                <!-- Llama-Nemotron: Efficient Reasoning Models (ICML 2025 - EXAIT Workshop) -->
                <tr>
                  <td style="padding:20px;width:45%;vertical-align:middle">
                    <img src="images/llama_nemotron.jpg" alt="Llama-Nemotron" width="100%" loading="lazy">
                  </td>
                  <td style="padding:20px;width:55%;vertical-align:middle">
                    <p class="papertitle">Llama-Nemotron: Efficient Reasoning Models</p>
                    <p>
                      Authors: Akhiad Bercovich · Itay Levy · Izik Golan · Mohammad Dabbah · Ran El-Yaniv · Omri Puny · Ido Galil · Zach Moshe · Tomer Ronen · Najeeb Nabwani · Ido Shahaf · Oren Tropp · Ehud Karpas · Ran Zilberstein · Jiaqi Zeng · Soumye Singhal · Alexander Bukharin · Yian Zhang · Tugrul Konuk · Gerald Shen · Ameya Sunil Mahabaleshwarkar · Bilal Kartal · Yoshi Suhara · Olivier Delalleau · Zijia Chen · Zhilin Wang · David Mosallanezhad · Adi Renduchintala · Haifeng Qian · Dima Rekesh
                      <a href="javascript:void(0);" onclick="toggleSummary('llama-nemotron-authors')" style="color:blue;text-decoration:underline;">Additional authors</a>
                    </p>
                    <div id="llama-nemotron-authors" style="display:none;margin-top:8px;">
                      <p>
                        Fei Jia · Somshubra Majumdar · Vahid Noroozi · Wasi Uddin Ahmad · Sean Narenthiran · Aleksander Ficek · Mehrzad Samadi · Jocelyn Huang · Siddhartha Jain · Igor Gitman · Ivan Moshkov · Wei Du · Shubham Toshniwal · George Armstrong · Branislav Kisacanin · Matvei Novikov · Daria Gitman · Evelina Bakhturina · Prasoon Varshney · Makesh Narsimhan · Jane Polak Scowcroft · John Kamalu · Dan Su · Kezhi Kong · Markus Kliegl · Rabeeh Karimi Mahabadi · Ying Lin · Sanjeev Satheesh · Jupinder Parmar · Pritam Gundecha · Brandon Norick · Joseph Jennings · Shrimai Prabhumoye · Syeda Nahida Akter · Mostofa Patwary · Abhinav Khattar · Deepak Narayanan · Roger Waleffe · Jimmy Zhang · Bor-Yiing Su · Guyue Huang · Terry Kong · Parth Chadha · Sahil Jain · Christine Harvey · Elad Segal · Jining Huang · Sergey Kashirsky · Robert McQueen · Izzy Putterman · George Lam · Arun Venkatesan · Sherry Wu · Vinh Nguyen · Manoj Kilaru · Andrew Wang · Anna Warno · Abhilash Somasamudramath · Sandip Bhaskar · Maka Dong · Nave Assaf · Shahar Mor · Omer Ullman Argov · Scot Junkin · Oleksandr Romanenko · Pedro Larroy · Monika Katariya · Marco Rovinelli · Viji Balas · Nicholas Edelman · Anahita Bhiwandiwalla · Muthu Subramaniam · Smita Ithape · Karthik Ramamoorthy · Yuting Wu · Suguna Varshini Velury · Omri Almog · Joyjit Daw · Denys Fridman · Erick Galinkin · Michael Evans · Shaona Ghosh · Katherine Luna · Leon Derczynski · Nikki Pope · Eileen Long · Seth Schneider · Guillermo Siman · Tomasz Grzegorzek · Pablo Ribalta · Monika Katariya · Chris Alexiuk · Joey Conway · Trisha Saar · Ann Guan · Krzysztof Pawelec · Shyamala Prayaga · Oleksii Kuchaiev · Boris Ginsburg · Oluwatobi Olabiyi · Kari Briski · Jonathan Cohen · Bryan Catanzaro · Jonah Alben · Yonatan Geifman · Eric Chung
                      </p>
                    </div>
                    <p>
                      <em>
                        <img src="images/ICML-logo.svg" alt="ICML" style="height:30px;vertical-align:middle;margin-right:5px;" loading="lazy">
                        ICML, 2025 - EXAIT Workshop
                        &nbsp;&nbsp;
                        <img src="images/nvidia_logo.jpg" alt="NVIDIA" style="height:30px;vertical-align:middle;margin-right:5px;" loading="lazy">
                      </em>
                    </p>
                    <p><strong>TL;DR</strong>: Llama‑Nemotron is a family of open reasoning LLMs (8B/49B/253B) that match state‑of‑the‑art reasoning quality while significantly improving inference throughput and memory efficiency, and include a dynamic reasoning toggle for controllable compute.</p>
                    <a href="javascript:void(0);" onclick="toggleSummary('llama-nemotron-summary')" style="color:blue;text-decoration:underline;">Read More</a>
                    <div id="llama-nemotron-summary" style="display:none;margin-top:10px;">
                      <p>
                        Llama‑Nemotron introduces heterogeneous reasoning models trained for both quality and efficiency. The recipe combines architecture search from Llama‑3 for faster inference, knowledge distillation and continued pretraining, followed by a reasoning‑focused post‑training stage (supervised fine‑tuning and large‑scale RL). The family (Nano 8B, Super 49B, Ultra 253B) achieves competitive reasoning vs. leading systems while improving throughput and memory use, and supports switching between standard chat and reasoning modes at inference time. The release includes models, a post‑training dataset, and training codebases (<a href="https://github.com/NVIDIA/NeMo" target="_blank" rel="noopener noreferrer">NeMo</a>, <a href="https://github.com/NVIDIA/NeMo-Aligner" target="_blank" rel="noopener noreferrer">NeMo‑Aligner</a>, <a href="https://github.com/NVIDIA/Megatron-LM" target="_blank" rel="noopener noreferrer">Megatron‑LM</a>).
                      </p>
                    </div>
                    <p>
                      <a href="https://arxiv.org/abs/2505.00949" target="_blank" rel="noopener noreferrer">Paper</a>
                    </p>
                  </td>
                </tr>

                <!-- No Data, No Optimization (Technion, 2025) -->
                <tr>
                  <td style="padding:20px;width:45%;vertical-align:middle">
                    <img src="images/dalmat.png" alt="No Data, No Optimization (thumbnail)" width="100%" loading="lazy">
                  </td>
                  <td style="padding:20px;width:55%;vertical-align:middle">
                    <p class="papertitle">No Data, No Optimization: A Lightweight Method To Disrupt Neural Networks With Sign-Flips</p>
                    <p>
                      Authors: Ido Galil* · Moshe Kimhi* · Ran El-Yaniv
                      &nbsp; (*Equal contribution)
                    </p>
                    <p>
                      <em>
                        2025
                        &nbsp;&nbsp;
                        <img src="images/technion_logo.png" alt="Technion" style="height:30px;vertical-align:middle;margin-left:15px;" loading="lazy">
                      </em>
                    </p>
                    <p><strong>TL;DR</strong>: We present a data-free, optimization-free attack that disrupts neural networks by flipping a tiny number of sign bits in their parameters. Flipping just two sign bits in ResNet-50 on ImageNet causes a 99.8% accuracy drop; a single-pass variant further amplifies damage.</p>
                    <a href="javascript:void(0);" onclick="toggleSummary('dnl-summary')" style="color:blue;text-decoration:underline;">Read More</a>
                    <div id="dnl-summary" style="display:none;margin-top:10px;">
                      <p>
                        Deep Neural Lesion (DNL) locates highly sensitive sign bits that control model behavior and flips them without any data or optimization. We validate effectiveness across diverse CV architectures and datasets, and show that a one-pass variant intensifies disruption beyond the zero-pass setting. Finally, hardening a small subset of vulnerable sign bits mitigates parameter attacks.
                      </p>
                    </div>
                    <p>
                      <a href="https://arxiv.org/abs/2502.07408" target="_blank" rel="noopener noreferrer">Paper</a>
                    </p>
                  </td>
                </tr>

                <!-- Hierarchical Selective Classification (NeurIPS 2024, Technion) -->
                <tr>
                  <td style="padding:20px;width:45%;vertical-align:middle">
                    <img src="images/hierarchical_selective_prediction.png" 
                         alt="Hierarchical Selective Classification" 
                         width="100%" loading="lazy">
                  </td>
                  <td style="padding:20px;width:55%;vertical-align:middle">
                    <p class="papertitle">Hierarchical Selective Classification</p>
                    <p>
                      Authors: Shani Goren* · Ido Galil* · Ran El-Yaniv 
                      &nbsp; (*Equal contribution)
                    </p>
                    <p>
                      <em>
                        <img src="images/neurips_logo.png" alt="NeurIPS" style="height:30px;vertical-align:middle;margin-right:5px;" loading="lazy">
                        NeurIPS, 2024
                        &nbsp;&nbsp;
                        <img src="images/technion_logo.png" alt="Technion" style="height:30px;vertical-align:middle;margin-left:15px;" loading="lazy">
                      </em>
                    </p>
                    <p><strong>TL;DR</strong>: We extend selective classification to a hierarchical setting, allowing models to reduce the specificity of predictions when uncertain.</p>
                    <a href="javascript:void(0);" onclick="toggleSummary('hsc-summary')" style="color:blue;text-decoration:underline;">Read More</a>
                    <div id="hsc-summary" style="display:none;margin-top:10px;">
                      <p>
                        Traditional selective classification only allows a full prediction or refusal. 
                        Our method uses class hierarchies to offer partial but valuable predictions (e.g., “malignant tumor” without specifying the subtype), improving calibration and risk-coverage trade-offs.
                      </p>
                    </div>
                    <p>
                      <a href="https://openreview.net/pdf?id=wzof7Y66xs" target="_blank" rel="noopener noreferrer">Paper</a> / 
                      <a href="https://www.youtube.com/watch?v=59CUrqKWxnw" target="_blank" rel="noopener noreferrer">Video</a> / 
                      <a href="https://github.com/shanigoren/Hierarchical-Selective-Classification" target="_blank" rel="noopener noreferrer">Code</a>
                    </p>
                  </td>
                </tr>
                <!-- C-OOD Benchmarking (ICLR 2023, Technion) -->
                <tr>
                  <td style="padding:20px;width:45%;vertical-align:middle">
                    <img src="images/degredation_graph_paper.png" 
                         alt="Degradation Graph" 
                         width="100%" loading="lazy">
                  </td>
                  <td style="padding:20px;width:55%;vertical-align:middle">
                    <p class="papertitle">
                      A Framework for Benchmarking Class-out-of-distribution Detection and its Application to ImageNet
                    </p>
                    <p>
                      Authors: <span>Ido Galil*</span> · <span>Mohammed Dabbah*</span> · <span>Ran El-Yaniv</span>
                      &nbsp; (*Equal contribution)
                    </p>
                    <p>
                      <em>
                        <img src="images/iclr_logo.png" alt="ICLR" 
                             style="height:30px;vertical-align:middle;margin-right:5px;" loading="lazy"> 
                        ICLR, 2023 (Top 25%)
                        &nbsp;&nbsp;
                        <img src="images/technion_logo.png" alt="Technion" style="height:30px;vertical-align:middle;margin-left:15px;" loading="lazy">
                      </em>
                    </p>
                    <p><strong>TL;DR</strong>: Introduces a new approach to generate multi-level C-OOD benchmarks for ImageNet classifiers, applied to 500+ models to reveal novel insights in open-set recognition.</p>
                    <a href="javascript:void(0);" onclick="toggleSummary('cood-summary')" style="color:blue;text-decoration:underline;">Read More</a>
                    <div id="cood-summary" style="display:none;margin-top:10px;">
                      <p>
                        Existing OOD benchmarks can be too easy or biased toward a particular model. 
                        Our framework systematically evaluates different detectors across multiple difficulty levels, uncovering how training regimes, architecture choices, and other factors influence performance.
                      </p>
                    </div>
                    <p>
                      <a href="https://openreview.net/pdf?id=Iuubb9W6Jtk" target="_blank" rel="noopener noreferrer">Paper</a> / 
                      <a href="https://www.youtube.com/watch?v=Q3XF06tcdDQ&t=1s" target="_blank" rel="noopener noreferrer">Video</a> / 
                      <a href="https://github.com/mdabbah/COOD_benchmarking" target="_blank" rel="noopener noreferrer">Code</a>
                    </p>
                  </td>
                </tr>

                <!-- 523 ImageNet Classifiers (ICLR 2023, Technion) -->
                <tr>
                  <td style="padding:20px;width:45%;vertical-align:middle">
                    <img src="images/Risk-Coverage_curve comparisonv3.png" 
                         alt="Risk Coverage Curve" 
                         width="100%" loading="lazy">
                  </td>
                  <td style="padding:20px;width:55%;vertical-align:middle">
                    <p class="papertitle">
                      What Can We Learn From the Selective Prediction and Uncertainty Estimation Performance of 523 ImageNet Classifiers?
                    </p>
                    <p>
                      Authors: Ido Galil · Mohammed Dabbah · Ran El-Yaniv
                    </p>
                    <p>
                      <em>
                        <img src="images/iclr_logo.png" alt="ICLR" 
                             style="height:30px;vertical-align:middle;margin-right:5px;" loading="lazy"> 
                        ICLR, 2023
                        &nbsp;&nbsp;
                        <img src="images/technion_logo.png" alt="Technion" style="height:30px;vertical-align:middle;margin-left:15px;" loading="lazy">
                      </em>
                    </p>
                    <p><strong>TL;DR</strong>: Extensive study on selective prediction and uncertainty estimation across 523 ImageNet models, highlighting that distillation and certain training regimes yield superior calibration and ranking.</p>
                    <a href="javascript:void(0);" onclick="toggleSummary('imagenet523-summary')" style="color:blue;text-decoration:underline;">Read More</a>
                    <div id="imagenet523-summary" style="display:none;margin-top:10px;">
                      <p>
                        Metrics such as AUROC, ECE, selective risk, and SAC show that knowledge distillation significantly improves uncertainty estimation. 
                        A subset of ViTs outperforms other architectures, and temperature scaling benefits both calibration and ranking performance more than previously realized.
                      </p>
                    </div>
                    <p>
                      <a href="https://openreview.net/pdf?id=p66AzKi6Xim" target="_blank" rel="noopener noreferrer">Paper</a> / 
                      <a href="https://www.youtube.com/watch?v=265cP8A80qY&t=1s" target="_blank" rel="noopener noreferrer">Video</a> / 
                      <a href="https://github.com/idogalil/benchmarking-uncertainty-estimation-performance" target="_blank" rel="noopener noreferrer">Code</a>
                    </p>
                  </td>
                </tr>

                <!-- Disrupting Deep Uncertainty (NeurIPS 2021, Technion) -->
                <tr>
                  <td style="padding:20px;width:45%;vertical-align:middle">
                    <img src="images/ACE_Intuition.png" 
                         alt="Disrupting Deep Uncertainty Estimation" 
                         width="100%" loading="lazy">
                  </td>
                  <td style="padding:20px;width:55%;vertical-align:middle">
                    <p class="papertitle">
                      Disrupting Deep Uncertainty Estimation Without Harming Accuracy
                    </p>
                    <p>
                      Authors: Ido Galil · Ran El-Yaniv
                    </p>
                    <p>
                      <em>
                        <img src="images/neurips_logo.png" alt="NeurIPS" 
                             style="height:30px;vertical-align:middle;margin-right:5px;" loading="lazy">
                        NeurIPS, 2021
                        &nbsp;&nbsp;
                        <img src="images/technion_logo.png" alt="Technion" 
                             style="height:30px;vertical-align:middle;margin-left:15px;" loading="lazy">
                      </em>
                    </p>
                    <p><strong>TL;DR</strong>: ACE (Attack on Confidence Estimation) disrupts a neural network’s uncertainty estimations without affecting its accuracy, making standard selective mechanisms unreliable.</p>
                    <a href="javascript:void(0);" onclick="toggleSummary('ace-summary')" style="color:blue;text-decoration:underline;">Read More</a>
                    <div id="ace-summary" style="display:none;margin-top:10px;">
                      <p>
                        Traditional adversarial attacks cross decision boundaries to harm accuracy. 
                        ACE selectively increases or decreases confidence for correct/incorrect predictions without crossing boundaries, 
                        rendering uncertainty estimates dangerous in sensitive scenarios.
                      </p>
                    </div>
                    <p>
                      <a href="https://papers.nips.cc/paper_files/paper/2021/file/b1b20d09041289e6c3fbb81850c5da54-Paper.pdf" target="_blank" rel="noopener noreferrer">Paper</a> / 
                      <a href="https://slideslive.com/38968191/disrupting-deep-uncertainty-estimation-without-harming-accuracy" target="_blank" rel="noopener noreferrer">Video</a> / 
                      <a href="https://github.com/IdoGalil/ACE" target="_blank" rel="noopener noreferrer">Code</a>
                    </p>
                  </td>
                </tr>

              </tbody>
            </table>

            <!-- Media / Interviews Section -->
            <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
              <tbody>
                <tr>
                  <td>
                    <h2>Media / Interviews</h2>
                  </td>
                </tr>
              </tbody>
            </table>
            <table width="100%" align="center" border="0" cellpadding="20">
              <tbody>
                <tr>
                  <td style="padding:20px;">
                    <p>
                      I was interviewed (in Hebrew) about my PhD research and teaching experience. 
                      You can listen to the interview on 
                      <a href="https://open.spotify.com/episode/4Xjnx09wRanVGp3781VPdR" target="_blank" rel="noopener noreferrer">Spotify</a>.
                    </p>
                  </td>
                </tr>
              </tbody>
            </table>

            <!-- Teaching Section -->
            <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
              <tbody>
                <tr>
                  <td>
                    <h2>Teaching</h2>
                  </td>
                </tr>
              </tbody>
            </table>
            <table width="100%" align="center" border="0" cellpadding="20">
              <tbody>
                <tr>
                  <td style="padding:20px;">
                    <p>
                      I served as a TA for the “Data Structures” course at the Technion for 3.5 years. 
                      All my tutorials and other helpful materials (in Hebrew) are available on my 
                      <a href="https://www.youtube.com/@idogalil" target="_blank" rel="noopener noreferrer">YouTube channel</a>.
                    </p>
                  </td>
                </tr>
              </tbody>
            </table>

            <!-- Awards & Honors Section -->
            <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
              <tbody>
                <tr>
                  <td>
                    <h2>Awards & Honors</h2>
                  </td>
                </tr>
              </tbody>
            </table>
            <table width="100%" align="center" border="0" cellpadding="20">
              <tbody>
                <tr>
                  <td style="padding:20px;">
                    <ul>
                      <li>Riva Dam Foundation Honors Scholarship for Excellence in PhD (2023)</li>
                      <li>Teaching Assistant Excellence Award (5 semesters)</li>
                      <li>Final's award for excellence in Computer Science (2018)</li>
                    </ul>
                  </td>
                </tr>
              </tbody>
            </table>

          </td>
        </tr>
      </tbody>
    </table>
  </body>
</html>
